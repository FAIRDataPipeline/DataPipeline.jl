var documenterSearchIndex = {"docs":
[{"location":"SEIRS/#SEIRS-model-example","page":"SEIRS model example","title":"SEIRS model example","text":"","category":"section"},{"location":"SEIRS/","page":"SEIRS model example","title":"SEIRS model example","text":"This is the updated manual for the upcoming FAIR interfaced version of the Data Registry package.","category":"page"},{"location":"SEIRS/","page":"SEIRS model example","title":"SEIRS model example","text":"run(`ls`)\nrun(`fair init --ci`)\nrun(`fair pull examples/fdp/SEIRSconfig.yaml`)","category":"page"},{"location":"SEIRS/","page":"SEIRS model example","title":"SEIRS model example","text":"dsf","category":"page"},{"location":"SEIRS/","page":"SEIRS model example","title":"SEIRS model example","text":"run(`fair run examples/fdp/SEIRSconfig.yaml`)","category":"page"},{"location":"examples/#Simple-example","page":"Simple example","title":"Simple example","text":"","category":"section"},{"location":"examples/#Introduction","page":"Simple example","title":"Introduction","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The purpose of this tutorial is to demonstrate use of the package to interact with the SCRC Data Registry (DR) in a process referred to as the 'pipeline', including:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"How to read data products from the DR,\nutilise them in a simple simulation model,\nregister the simulation model code with the DR,\nand register the simulation model run with the DR.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The example is also provided as working code (including the accompanying configuration files) in the examples/simple directory of the package repository. The steps and the corresponding line of code in that module are:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Steps / Learning outcomes Code\n1. Preliminaries L24\n2. Config files and scripts L32\n3. Reading data products from the Registry L43\n4. Running a model simulation L65\n4b. Automatic data access logging L103\n5. Staging 'code repo releases' (i.e. model code) L107\n6. Staging model 'code runs' L111\n7. Committing staged objects to the Registry L133","category":"page"},{"location":"examples/#.-Package-installation","page":"Simple example","title":"0. Package installation","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The package is not currently registered and must be added via the package manager Pkg. From the REPL type ] to enter Pkg mode and run:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"pkg> add https://github.com/FAIRDataPipeline/DataPipeline.jl","category":"page"},{"location":"examples/#.-Preliminaries:-import-packages","page":"Simple example","title":"1. Preliminaries: import packages","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"import DataPipeline    # pipeline stuff\nimport DPOMPs               # simulation of epidemiological models\nimport YAML                 # for reading model config file\nimport Random               # other assorted packages used incidentally\nimport DataFrames","category":"page"},{"location":"examples/#.-Specify-config-files,-scripts-and-data-directory","page":"Simple example","title":"2. Specify config files, scripts and data directory","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"These variables and the corresponding files determine the model configuration; data products to be downloaded; and 'submission script' (see 2c.)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"model_config = \"/examples/simple/model_config.yaml\"     # (see 2a)\ndata_config = \"/examples/simple/data_config.yaml\"       # (see 2b)\nsubmission_script = \"julia examples/simple/main.jl\"     # (see 2c)","category":"page"},{"location":"examples/#a.-The-*model_config.yaml*-file","page":"Simple example","title":"2a. The model_config.yaml file","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The 'model config' file concept is used throughout the SCRC data pipeline, (i.e. not just within this package.) In this example, it is used to store information about both the model code (step 3,) and the individual code run (step 6.) The example below is also given here.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"# model\nmodel_name: \"DataPipeline simple example\"\nmodel_repo: \"https://github.com/FAIRDataPipeline/DataPipeline.jl\"\n# NB. ^ because the example is part of this package - replace with your own repo\nmodel_version: \"0.0.4\"\nmodel_description: \"A simple SEIR simulation for demonstrating use of the DataPipeline.jl package.\"\nmodel_website: \"https://mjb3.github.io/DiscretePOMP.jl/stable/\"\n\n# simulation parameters\nrandom_seed: 1\ninitial_s: 1000   # initial population size\nmax_t: 180.0      # simulation time\nbeta: 0.7         # contact rate := beta SI / N","category":"page"},{"location":"examples/#b.-The-*data_config.yaml*-file","page":"Simple example","title":"2b. The data_config.yaml file","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Similar to the model configuration file, 'data config' files are a standard way to interact with the data pipeline, including in other languages besides Julia. This example specifies the Data Products that are downloaded in step 4:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"fail_on_hash_mismatch: True     # set 'False' to suppress data mismatch errors\nnamespace: SCRC                 # default namespace\n\nread:\n  - where:\n      data_product: human/infection/SARS-CoV-2/symptom-probability\n      component: symptom-probability\n  - where:\n      data_product: prob_hosp_and_cfr/data_for_scotland\n      component: cfr_byage\n    use:\n      namespace: EERA\n  - where:\n      data_product: human/infection/SARS-CoV-2/asymptomatic-period\n      component: asymptomatic-period\n  - where:\n      data_product: human/infection/SARS-CoV-2/infectious-duration\n      component: infectious-duration\n  - where:\n      data_product: human/infection/SARS-CoV-2/latent-period\n      component: latent-period\n  - where:\n      data_product: fixed-parameters/T_hos\n      component: T_hos\n    use:\n      namespace: EERA\n  - where:\n      data_product: fixed-parameters/T_rec\n      component: T_rec\n    use:\n      namespace: EERA","category":"page"},{"location":"examples/#c.-The-*submission_script*-variable","page":"Simple example","title":"2c. The submission_script variable","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Finally, the submission_script variable is a string that contains the contents of the 'submission script' file; another artefact of the pipeline process that applies outwith the Julia package.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"submission_script = \"julia examples/simple/main.jl\"","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Here it is used to define the 'entry point' of the application; together with the model code and 'config' files, it will allow others to reproduce our results in the future with ease and precision (an important benefit of the overall pipeline process.)","category":"page"},{"location":"examples/#.-Downloading-data-products","page":"Simple example","title":"3. Downloading data products","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Here we read some epidemiological parameters from the DR, so we can use them to run an SEIR simulation in step (5). First we download some data, then read it.","category":"page"},{"location":"examples/#a.-Download-data","page":"Simple example","title":"3a. Download data","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"First, we process the data_config file, which (in this case) returns a variable representing a connection to a SQLite database. I.e. we download the data products:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"data_dir = \"/examples/simple/data/\" # local directory where data is to be stored\ndb = DataPipeline.initialise_local_registry(data_dir, data_config=data_config)","category":"page"},{"location":"examples/#b.-Read-some-data","page":"Simple example","title":"3b. Read some data","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Next, we read some parameters and convert them to the required units.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"inf_period_days = DataPipeline.read_estimate(db, \"human/infection/SARS-CoV-2/\", \"infectious-duration\", key=\"value\", data_type=Float64)[1] / 24\nlat_period_days = DataPipeline.read_estimate(db, \"human/infection/SARS-CoV-2/\", \"latent-period\", key=\"value\", data_type=Float64)[1] / 24","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"See Code snippets and the Package manual for information about reading other types of data product.","category":"page"},{"location":"examples/#c.-Offline-access","page":"Simple example","title":"3c. Offline access","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Once the data has been downloaded initially, it can be retrieved for offline access (e.g. no or slow internet connections) using the offline_mode option:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"db = DataPipeline.initialise_local_registry(data_dir, data_config=data_config, auto_logging=true, offline_mode=true)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The main functions for registering objects, such as model code, model runs, and data products, work by staging the data to a local database so they are also compatible with 'offline mode'. only the commit_ functions require internet access, which can be run once access has been restored.","category":"page"},{"location":"examples/#.-Model-simulation","page":"Simple example","title":"4. Model simulation","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Step 5 relies on the use of another package: DiscretePOMP.jl, so you may wish to skip this section or replace it with, e.g. your own model or simulation code.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Next, for illustration purposes only we run a simple SEIR simulation using the Gillespie simulation feature of the DiscretePOMP.jl package. We use the downloaded parameters* as inputs, and finally plot the results as a time series of the population as they migrate between states according to the stochastic dynamics of the model.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"First we extract some information about the model run from the model_config.yaml file:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"mc = YAML.load_file(model_config)\np = mc[\"initial_s\"]   # population size\nt = mc[\"max_t\"]       # simulation time\nbeta = mc[\"beta\"]     # nb. contact rate := beta SI / N\nRandom.seed!(mc[\"random_seed\"])","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"These include the population size and contact parameter beta, as well as the random seed. We are then ready the generate a DiscretePOMP model:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"## define a vector of simulation parameters\ntheta = [beta, inf_period_days^-1, lat_period_days^-1]\n## initial system state variable [S E I R]\ninitial_condition = [p - 1, 0, 1, 0]\n## generate DPOMPs model (see https://github.com/mjb3/DiscretePOMP.jl)\nmodel = DiscretePOMP.generate_model(\"SEIR\", initial_condition, freq_dep=true)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Finally, we run the simulation and plot the results:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"x = DiscretePOMP.gillespie_sim(model, theta, tmax=t)\nprintln(DiscretePOMP.plot_trajectory(x))","category":"page"},{"location":"examples/#b.-automatic-data-access-logging","page":"Simple example","title":"4b. automatic data access logging","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"In order to register a given code run in the DR, we require a record of the data products and components accessed to produce that particular run. Data access logging can essentially be handled automatically. The most recent active data log is used to register code runs by default. However, it is advisable to manually 'finish' the log instead, and specify the log id manually at the point of registration:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"## this is not usually necessary in a single process environment:\n# initialise_data_log(db, offline_mode)\n\n## this is necessary when we want a [human-*and*-machine-readable] file copy of the log\nlog_id = DataPipeline.finish_data_log(db; filepath=data_log)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"This is particularly important if we are planning on doing further work in the interim, such as if we want an accurate record of the access time. Logs should also handled manually (including initialisation) when working with multiple processes running on separate cores, but utilising the same filesystem/data resources.","category":"page"},{"location":"examples/#.-Registering-model-code","page":"Simple example","title":"3. Registering model code","text":"","category":"section"},{"location":"examples/#a.-SCRC-access-token-request-via-https://data.scrc.uk/docs/","page":"Simple example","title":"3a. SCRC access token - request via https://data.scrc.uk/docs/","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"An access token is required if you want to write to the DR (e.g. register model code / runs) but not necessary if you only want to read from the DR (e.g. download data products.)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The token must not be shared. Common approaches include the use of system variables or [private] configuration files. In this example I have included mine as a separate Julia file with a single line of code. Note that it is important to also specify the .gitignore so as not to accidentally upload to the internet!","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"include(\"access-token.jl\")","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The variable itself looks like: \"token [my token]\". For example, if the token is the numbers one through six, the access-token.jl file looks like this:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"const scrc_access_tkn = \"token 123456\"","category":"page"},{"location":"examples/#b.-Register-model-code","page":"Simple example","title":"3b. Register model code","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Back in the main file, we handle model code [release] registration by calling a function that automatically returns the existing code_repo_release URI if it is already registered, or a new one if not.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"code_release_id = DataPipeline.register_github_model(model_config, scrc_access_tkn)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Here we have used a .yaml configuration file but for illustration, the code is roughly equivalent to this:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"model_name = \"DataPipeline simple example\"\nmodel_repo = \"https://github.com/FAIRDataPipeline/DataPipeline.jl\"\nmodel_version = \"0.0.1\"\nmodel_description = \" ... \" (nb. insert description)\nmodel_docs = \"https://mjb3.github.io/DiscretePOMP.jl/stable/\"\ncode_release_id = DataPipeline.register_github_model(model_name, model_version, model_repo, model_hash, scrc_access_tkn, model_description=model_description, model_website=model_docs)","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Finally, the resulting URI is in the form:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"code_release_id := \"https://data.scrc.uk/api/code_repo_release/2157/\"","category":"page"},{"location":"examples/#.-Registering-a-'model-run'","page":"Simple example","title":"6. Registering a 'model run'","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Next we register the results of this particular simulation by POSTing to the code_run endpoint of the DR's RESTful API:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"model_run_description = \"Just another SEIR simulation.\"\nmodel_run_id = DataPipeline.register_model_run(model_config, submission_script, code_release_id, model_run_description, scrc_access_tkn)","category":"page"},{"location":"examples/#.-Finally,-commit-objects-to-registry","page":"Simple example","title":"7. Finally, commit objects to registry","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"The final step is to commit the local changes we have made to the DR:","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"code_release_url = DataPipeline.commit_staged_model(db, code_release_id, scrc_access_tkn)\nmodel_run_url = DataPipeline.commit_staged_run(db, model_run_id, scrc_access_tkn)\nDataPipeline.registry_commit_status(db)    # optional: display status","category":"page"},{"location":"examples/#Finished!","page":"Simple example","title":"Finished!","text":"","category":"section"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"That concludes the example. A complete working example of this code can be found here.","category":"page"},{"location":"examples/","page":"Simple example","title":"Simple example","text":"Please note that certain features, notably the registration of Data Products (i.e. model 'inputs' and 'outputs') is currently still a work in progress. See the home page for more information.","category":"page"},{"location":"snip/#Code-snippets","page":"Code snippets","title":"Code snippets","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"Pages = [\"snip.md\"]\nDepth = 3","category":"page"},{"location":"snip/#Getting-started-package-installation","page":"Code snippets","title":"Getting started - package installation","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"The package is now registered in General, so can be added via the package manager Pkg. From the REPL type ] to enter Pkg mode and run:","category":"page"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"pkg> add DataPipeline","category":"page"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"using DataPipeline\n?fetch_data_per_yaml","category":"page"},{"location":"snip/#Usage","page":"Code snippets","title":"Usage","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"It is recommended to use a .yaml data configuration file to specify the data products to be downloaded. Some example .yaml file are included in the examples folder. Refer to https://data.scrc.uk/ for information about other data products available in the registry.","category":"page"},{"location":"snip/#Example:-refreshing-(i.e.-downloading)-data","page":"Code snippets","title":"Example: refreshing (i.e. downloading) data","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"TEST_FILE = \"examples/data_config.yaml\"\nDATA_OUT = \"out/\"\ndata = DataPipeline.fetch_data_per_yaml(TEST_FILE, DATA_OUT)","category":"page"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"The results referenced by the data variable is a SQLite file databased, containing records of downloaded data products, components, and so on. They can be accessed thusly:","category":"page"},{"location":"snip/#Example:-reading-key-value-pairs,-e.g.-point-estimates","page":"Code snippets","title":"Example: reading key-value pairs, e.g. point estimates","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"data_product = \"human/infection/SARS-CoV-2/infectious-duration\"\ncomp_name = \"infectious-duration\"\n# by data product\nest = DataPipeline.read_estimate(data, data_product)\n# by component name\nest = DataPipeline.read_estimate(data, data_product, comp_name)","category":"page"},{"location":"snip/#Example:-reading-arrays","page":"Code snippets","title":"Example: reading arrays","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"data_product = \"records/SARS-CoV-2/scotland/cases_and_management\"\ncomp_name = \"/test_result/date-cumulative\"\n## read array by dp:\nsome_arrays = DataPipeline.read_array(data, data_product)\none_array = some_arrays[comp_name]\n## read array by component name:\none_array = DataPipeline.read_array(data, data_product, comp_name)","category":"page"},{"location":"snip/#Example:-reading-tables","page":"Code snippets","title":"Example: reading tables","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"data_product = \"geography/scotland/lookup_table\"\ncomp_name = \"/conversiontable/scotland\"\ntbl = DataPipeline.read_table(data, data_product, comp_name)","category":"page"},{"location":"snip/#Example:-reading-data-products-from-file","page":"Code snippets","title":"Example: reading data products from file","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"You can also use the package to read in a file that has already been downloaded, as follows:","category":"page"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"fp = \"/path/to/some/file.h5\"\ndp = DataPipeline.read_data_product_from_file(fp, use_axis_arrays = true, verbose = false)\ncomponent = dp[\"/conversiontable/scotland\"]","category":"page"},{"location":"snip/#Example:-custom-SQL-query","page":"Code snippets","title":"Example: custom SQL query","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"Data can also be queried using SQL for convenient joining and aggregation. For example:","category":"page"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"using SQLite, DataFrames\ndb = DataPipeline.read_data_product(fp, use_sql = true)\nx = DBInterface.execute(data, \"SELECT * FROM data_product\") |> DataFrame","category":"page"},{"location":"snip/#What's-my-file?","page":"Code snippets","title":"What's my file?","text":"","category":"section"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"Sometimes you need to know if a file (or directory of files,) is registered in the Data Registry. This can be accomplished using the whats_my_file function. For example:","category":"page"},{"location":"snip/","page":"Code snippets","title":"Code snippets","text":"DataPipeline.whats_my_file(\"path/to/some/file/or/directory\")","category":"page"},{"location":"manual/#Package-manual","page":"Package manual","title":"Package manual","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"Pages = [\"manual.md\"]\nDepth = 3","category":"page"},{"location":"manual/#Reading-/-downloading-data","page":"Package manual","title":"Reading / downloading data","text":"","category":"section"},{"location":"manual/#Downloading-data-products","page":"Package manual","title":"Downloading data products","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"Note that data products are processed and downloaded at the point of initialisation, provided that a data_config file is specified, and the offline_mode option is not used.","category":"page"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"initialise_local_registry","category":"page"},{"location":"manual/#Reading-data","page":"Package manual","title":"Reading data","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"Note that arrays can be processed into flat 2d tables, using either load_array! or read_array with the flatten=true.","category":"page"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"read_estimate\nread_array\nload_array!\nread_table\nread_data_product_from_file","category":"page"},{"location":"manual/#Writing-to-the-Data-Registry","page":"Package manual","title":"Writing to the Data Registry","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"The process of registering objects such as data, code, and model runs, in the main Data Registry involves two steps; [local] registration, and then committing registered objects to the main online Registry.","category":"page"},{"location":"manual/#Registering-objects-locally","page":"Package manual","title":"Registering objects locally","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"register_data_product\nregister_github_model\nregister_model_run","category":"page"},{"location":"manual/#DataPipeline.register_data_product","page":"Package manual","title":"DataPipeline.register_data_product","text":"register_data_product(handle, data_product)\n\nRegister data product (from link_write())\n\n\n\n\n\n","category":"function"},{"location":"manual/#Committing-objects-to-the-online-Registry","page":"Package manual","title":"Committing objects to the online Registry","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"Note that 'staged' objects (i.e. registered locally) can be committed all at once using commit_all, or one at a time using the identifiers yielded by the above function calls, e.g. register_data_product.","category":"page"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"registry_commit_status\ncommit_all","category":"page"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"commit_staged_data_product\ncommit_staged_model\ncommit_staged_run","category":"page"},{"location":"manual/#Other","page":"Package manual","title":"Other","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"whats_my_file\nregistry_audit","category":"page"},{"location":"manual/#Index","page":"Package manual","title":"Index","text":"","category":"section"},{"location":"manual/","page":"Package manual","title":"Package manual","text":"","category":"page"},{"location":"fdp_manual/#FAIR-data-pipeline-manual","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"This is the updated manual for the upcoming FAIR interfaced version of the Data Registry package.","category":"page"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"Pages = [\"fdp_manual.md\"]\nDepth = 3","category":"page"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"Note that data products are processed and downloaded at the point of initialisation, provided that a data_config file is specified, and the offline_mode option is not used.","category":"page"},{"location":"fdp_manual/#Managing-code-runs","page":"FAIR data pipeline manual","title":"Managing code runs","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"initialise\nfinalise","category":"page"},{"location":"fdp_manual/#DataPipeline.initialise","page":"FAIR data pipeline manual","title":"DataPipeline.initialise","text":"initialise(config_file, submission_script)\n\nRead [working] config.yaml file. Returns a DataRegistryHandle containing:\n\nthe working config.yaml file contents\nthe object id for this file\nthe object id for the submission script file\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.finalise","page":"FAIR data pipeline manual","title":"DataPipeline.finalise","text":"finalise(handle)\n\nComplete (i.e. finish) code run.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#Reading-data","page":"FAIR data pipeline manual","title":"Reading data","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"read_array\nread_table\nread_estimate\nread_distribution\nlink_read","category":"page"},{"location":"fdp_manual/#DataPipeline.read_array","page":"FAIR data pipeline manual","title":"DataPipeline.read_array","text":"read_array(handle, data_product[, component])\n\nRead [array] data product.\n\nnote that it must already have been downloaded from the remote data store using fdp pull.\nthe latest version of the data is read unless otherwise specified.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.read_table","page":"FAIR data pipeline manual","title":"DataPipeline.read_table","text":"read_table(handle, data_product[, component])\n\nRead [table] data product.\n\nnote that it must already have been downloaded from the remote data store using fdp pull.\nthe latest version of the data is read unless otherwise specified.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.read_estimate","page":"FAIR data pipeline manual","title":"DataPipeline.read_estimate","text":"read_estimate(handle, data_product, [component])\n\nRead TOML-based data product.\n\nnote that it must already have been downloaded from the remote data store using fdp pull.\nthe specific version can be specified in the config file (else the latest version is used.)\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.read_distribution","page":"FAIR data pipeline manual","title":"DataPipeline.read_distribution","text":"readdistribution(handle, dataproduct, [component])\n\nRead TOML-based data product.\n\nnote that it must already have been downloaded from the remote data store using fdp pull.\nthe specific version can be specified in the config file (else the latest version is used.)\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.link_read","page":"FAIR data pipeline manual","title":"DataPipeline.link_read","text":"link_read(handle, data_product)\n\nReturns the file path of a data product that has been registered in the local data registry, either directly or via the CLI.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#Writing-to-the-Data-Registry","page":"FAIR data pipeline manual","title":"Writing to the Data Registry","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"The process of registering objects such as data, code, and model runs, in the main Data Registry involves two steps; [local] registration, and then committing registered objects to the main online Registry.","category":"page"},{"location":"fdp_manual/#Registering-data","page":"FAIR data pipeline manual","title":"Registering data","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"write_array\nwrite_table\nwrite_estimate\nwrite_distribution\nlink_write","category":"page"},{"location":"fdp_manual/#DataPipeline.write_array","page":"FAIR data pipeline manual","title":"DataPipeline.write_array","text":"write_array(handle, data, data_product, component)\n\nWrite an array as a component to an hdf5 file.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.write_table","page":"FAIR data pipeline manual","title":"DataPipeline.write_table","text":"write_table(handle, data, data_product, component)\n\nWrite a table as a component to an hdf5 file.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.write_estimate","page":"FAIR data pipeline manual","title":"DataPipeline.write_estimate","text":"write_estimate(handle, value, data_product, component)\n\nWrite a point estimate as a component to a toml file.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.write_distribution","page":"FAIR data pipeline manual","title":"DataPipeline.write_distribution","text":"write_distribution(handle, distribution, parameters, data_product, component)\n\nWrite a distribution as a component to a toml file.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#DataPipeline.link_write","page":"FAIR data pipeline manual","title":"DataPipeline.link_write","text":"linkwrite(handle, dataproduct)\n\nRegisters a file-based data product based on information provided in the working config file, e.g. for writing external objects.\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#Raising-issues","page":"FAIR data pipeline manual","title":"Raising issues","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"raise_issue","category":"page"},{"location":"fdp_manual/#DataPipeline.raise_issue","page":"FAIR data pipeline manual","title":"DataPipeline.raise_issue","text":"raise_issue(handle; ... )\n\nRegister issue with data product; component; external object; or script.\n\nPass the object URI as a named parameter[s], e.g. raise_issue(handle; data_product=dp, component=comp).\n\nOptional parameters\n\ndata_product\ncomponent\nexternal_object\nscript\n\n\n\n\n\n","category":"function"},{"location":"fdp_manual/#What's-my-file?","page":"FAIR data pipeline manual","title":"What's my file?","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"whats_my_file\nregistry_audit","category":"page"},{"location":"fdp_manual/#Index","page":"FAIR data pipeline manual","title":"Index","text":"","category":"section"},{"location":"fdp_manual/","page":"FAIR data pipeline manual","title":"FAIR data pipeline manual","text":"Pages = [\"fdp_manual.md\"]","category":"page"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"note: Note\nSee here for the main FAIR Data Pipeline documentation, and information about the SCRC. This website is for the Julia package only.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"DataPipeline.jl  -  the SCRC 'Data Pipeline' in Julia","category":"page"},{"location":"#What-is-the-SCRC-data-pipeline?","page":"Introduction","title":"What is the SCRC data pipeline?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Per the SCRC docs, the Scottish COVID-19 Response Consortium [SCRC] is a research consortia concisting \"of dozens of individuals from over 30 academic and commercial organisations\" formed in response to RAMP: Rapid Assistance in Modelling the Pandemic, a directive to the scientific community coordinated by the Royal Society.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"A key outcome of the project is to develop more epidemiological models of COVID-19 spread in order to develop a more robust and clearer understanding of the impacts of different exit strategies from lockdown - see the SCRC docs for more information.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"As a working process developed by the SCRC, the data pipeline can be understood by considering the central kernel of its technological implementation: the Data Registry (DR). Essentially it consists of a relational database, and a RESTful API for reading and writing to the database.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The database schema (as illustrated below) is detailed, but key entity types of relevance here include:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Data Products - metadata, or information about data 'products'. To elaborate: a data product typically includes a link to, e.g. a table of scientific data, but [for the most part] the underlying data is not actually stored in the DR. This may appear at first glance to be a limitation but there is a key benefit to the approach which is discussed briefly in due course.\nCode Repo Releases - i.e. 'models', or a given version of some code that implements, e.g. a statistical model.\nCode runs - or model runs, such as the output from a single realisation of the model.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Thus, the Code Repo (and Release) relating to a given statistical model, may be associated with a number of Code Runs, which in turn may be associated with a number of Data Products (as 'inputs' and 'outputs'.)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In summary the data pipeline provides both a centralised repository for [meta]data, and a means of tracking the full history of COVID-related modelling outputs, including data and other inputs, such as the random seed used to generate a particular realisation of a given model.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The resulting 'audit trail' can thus provide transparency, and greatly improve the reproducibility, of published scientific research, even where the models and data used to produce them are complex or sophisticated.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Note that as a working process the pipeline is somewhat cyclical in nature: model outputs can be used to provide inputs for other models, and so on. Thus the audit capabilities of the pipeline process are not limited to individual research projects, models or datasets - it naturally extends to a sequence of ongoing projects, possibly produced by different users and teams ('groups' in the DR.)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In other words, it mirrors the way in which scientific research in general is published, whilst providing a robust solution to vitally important current challenges far beyond the fields of public health and epidemiology, such as reproducibility and transparency.","category":"page"},{"location":"#The-SCRC-Data-Registry","page":"Introduction","title":"The SCRC Data Registry","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"As with the pipeline itself, a key design strength of the Data Registry (DR) is its 'agnosticism'. That is, it is agnostic with respect to both programming languages and [the format of] datasets. Thus, it is compatible even with those that have not been invented yet.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Whilst this is a key strength, it does impose certain constraints on the functionality that can be provided directly within the framework of the DR itself. For example, in order to provide features such as file processing (necessary for the automated import of Data Products from the DR) it is necessary to know the file format in advance. This is also true of data processing in general: in order to do any kind of meaningful data processing, we must know both the structure of a given type of dataset, and how to recognise it in practice.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"For that reason, features such as these are instead provided by what can be regarded as an 'automation layer', a set of utility-like software packages (such as this one) and tools that comprise an important layer of the pipline's software ecosystem, because they make it possible for model developers to download data, and otherwise meaningfully interact with the SCRC pipeline process using only a few lines of code.","category":"page"},{"location":"#Data-Registry-schema","page":"Introduction","title":"Data Registry schema","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"<img src=\"https://data.scrc.uk/static/images/schema.svg\" alt=\"SCRC Data Registry schema=\"height: 80px;\"/>","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Hint: click here to expand the image.","category":"page"},{"location":"#What-does-this-package-do?","page":"Introduction","title":"What does this package do?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Similar to the SCRCData package for R and the datapipelineapi package for Python, this package provides a language-specific automation layer [for the language-agnostic RESTful API that is used to interact with the DR.] It also handles the downloading (and pre-processing) of Data Products based on that [meta]data.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Key features include:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Convenient Data Products downloads, specified by a given 'data config' .yaml file.\nFile hash-based version checking: new data is downloaded only when necessary.\nA SQLite layer for convenient pre-processing (typically aggregation, and the joining of disparate datasets based on common identifiers.)\nEasily register model code or realisations with a single line of code.","category":"page"},{"location":"#SQLite-layer","page":"Introduction","title":"SQLite layer","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The SQLite layer is optional - the data can also be returned as a set of nested Dictionaries. However it is the recommended way of using the package since it provides for convenient pre-processing and as well as certain planned features of the package. See the SQLite.jl docs for more information about that package, and the Simple example and Code snippets page for practical usage examples pertinent to this one.","category":"page"},{"location":"#Getting-started","page":"Introduction","title":"Getting started","text":"","category":"section"},{"location":"#Package-installation","page":"Introduction","title":"Package installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The package is not currently registered and must be added via the package manager Pkg. From the REPL type ] to enter Pkg mode and run:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add https://github.com/FAIRDataPipeline/DataPipeline.jl","category":"page"},{"location":"#Further-development-work","page":"Introduction","title":"Further development work","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"There is ongoing development work to do, subject to feedback from users. In particular:","category":"page"},{"location":"#Registering-data-products","page":"Introduction","title":"Registering data products","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The registration of Data Products, including but not limited to model outputs, in the DR is a WIP.","category":"page"},{"location":"#SQL-based-access-logs-(a-component-of-Code-Run-objects)","page":"Introduction","title":"SQL-based access logs (a component of Code Run objects)","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Existing functionality for recording data usage in model runs is based on the individual data products specified in the data configuration .yaml file. However since data products may include multiple components, it would be better to have a more precise record of the data that is actually utilised. This functionality is currently being implemented via the aforementioned SQLite layer.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Features in consideration include optional 'tags' specified by the user at the point of [data] access - passed as a parameter to a given function call. It is anticipated that these tags could be used to record information such as the calling Julia Module / line of code; key filters and aggregation levels used to process the data; or even references to downstream outputs, e.g. 'Page x, Table y in Report z.'","category":"page"},{"location":"#Suggestions-welcome!","page":"Introduction","title":"Suggestions welcome!","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Feel free to reach out: martin.burke@bioss.ac.uk","category":"page"}]
}
